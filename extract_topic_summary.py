# # from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModel
# # import nltk
# # import torch
# # import torch.nn.functional as F

# # # NLTK ë‹¤ìš´ë¡œë“œ
# # nltk.download('punkt')

# # # KoBART ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ
# # model_t5_dir = "lcw99/t5-base-korean-text-summary"
# # model_KoT5_dir = "noahkim/KoT5_news_summarization"
# # tokenizer_t5 = AutoTokenizer.from_pretrained(model_t5_dir)
# # tokenizer_KoT5 = AutoTokenizer.from_pretrained(model_KoT5_dir)
# # model_t5 = AutoModelForSeq2SeqLM.from_pretrained(model_t5_dir)
# # model_KoT5 = AutoModelForSeq2SeqLM.from_pretrained(model_KoT5_dir)

# # max_input_length = 2048


# # class KoBERTSimilarity:
# #     def __init__(self, model_name="skt/kobert-base-v1"):
# #         self.device = torch.device(
# #             "cuda" if torch.cuda.is_available() else "cpu")
# #         self.tokenizer = AutoTokenizer.from_pretrained(model_name)
# #         self.model = AutoModel.from_pretrained(model_name).to(self.device)
# #         self.model.eval()

# #     def get_embedding(self, text):
# #         inputs = self.tokenizer(text, return_tensors="pt", truncation=True,
# #                                 padding=True, max_length=64).to(self.device)
# #         if "token_type_ids" in inputs:
# #             del inputs["token_type_ids"]  # kobert ì—ëŸ¬ ë°©ì§€
# #         with torch.no_grad():
# #             outputs = self.model(**inputs)
# #             # [CLS] í† í°ì˜ ì„ë² ë”©
# #             return outputs.last_hidden_state[:, 0, :].squeeze(0)

# #     def similarity(self, text1, text2):
# #         emb1 = self.get_embedding(text1)
# #         emb2 = self.get_embedding(text2)
# #         return F.cosine_similarity(emb1, emb2, dim=0).item()

# #     def most_similar(self, title, candidates):
# #         best_score = -1
# #         best_text = ""
# #         for c in candidates:
# #             score = self.similarity(title, c)
# #             if score > best_score:
# #                 best_score = score
# #                 best_text = c
# #         return best_text, best_score


# # def get_summary_t5(text, max_length=100):
# #     """
# #     ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ì— ëŒ€í•´ ìš”ì•½ì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜
# #     :param text: ìš”ì•½í•  í…ìŠ¤íŠ¸
# #     :param max_length: ìš”ì•½ë¬¸ ìµœëŒ€ ê¸¸ì´
# #     :return: ìš”ì•½ëœ í…ìŠ¤íŠ¸
# #     """
# #     inputs = [text]
# #     inputs = tokenizer_t5(inputs, max_length=max_input_length,
# #                           truncation=True, return_tensors="pt", padding=True)
# #     output = model_t5.generate(
# #         **inputs, num_beams=16, do_sample=False, min_length=1, max_length=max_length)
# #     decoded_output = tokenizer_t5.batch_decode(
# #         output, skip_special_tokens=True)[0]
# #     topic = nltk.sent_tokenize(decoded_output.strip())[0]
# #     return topic


# # def get_summary_KoT5(text, max_length=100):
# #     """
# #     ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ì— ëŒ€í•´ ìš”ì•½ì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜
# #     :param text: ìš”ì•½í•  í…ìŠ¤íŠ¸
# #     :param max_length: ìš”ì•½ë¬¸ ìµœëŒ€ ê¸¸ì´
# #     :return: ìš”ì•½ëœ í…ìŠ¤íŠ¸
# #     """
# #     inputs = [text]
# #     inputs = tokenizer_KoT5(inputs, max_length=max_input_length,
# #                             truncation=True, return_tensors="pt", padding=True)
# #     output = model_KoT5.generate(
# #         **inputs, num_beams=16, do_sample=False, min_length=1, max_length=max_length)
# #     decoded_output = tokenizer_KoT5.batch_decode(
# #         output, skip_special_tokens=True)[0]
# #     topic = nltk.sent_tokenize(decoded_output.strip())[0]
# #     return topic


# # def devide_paragraph(body):
# #     paragraphs = body.split("\n")
# #     return paragraphs


# # def extract_topic(text):
# #     paragraphs = devide_paragraph(text)
# #     # summary = get_summary_KoT5(text, max_length=128)
# #     summary = get_summary_t5(text, max_length=128)
# #     print(summary)
# #     return summary


# # if __name__ == "__main__":
# #     body = """
# #     ê³ ë¯¼ì • ìµœê³ ìœ„ì›ì€ CBS ë¼ë””ì˜¤ì—ì„œ ì²´í¬ë™ì˜ì•ˆ ë¶€ê²° ì£¼ì¥ì— ëŒ€í•´ ì–¸ê¸‰í–ˆë‹¤.
# #     """

# #     # ìš”ì•½ ì‹¤í–‰
# #     final_summary = get_summary(body)

# #     # ê²°ê³¼ ì¶œë ¥
# #     print("ìµœì¢… ìš”ì•½:", final_summary)

# # # ~ë©°, ~ë©´ìœ¼ë¡œ ë¶„ë¦¬
# # # ~í–ˆê³  ë¶„ë¦¬
# # # ë•Œë¬¸ì—


# # import re
# # import torch
# # import torch.nn.functional as F
# # import stanza
# # from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModel
# # import nltk

# # # NLTK & Stanza ë‹¤ìš´ë¡œë“œ
# # nltk.download("punkt")
# # stanza.download("ko")

# # # Stanza ì´ˆê¸°í™”
# # nlp = stanza.Pipeline(lang='ko', processors='tokenize,pos,lemma,depparse')


# # class SentenceSplitter:
# #     CONNECTIVE_PATTERN = r"(ë©´ì„œ|ë©°|í•˜ë©°|ë¼ë©°|í•˜ê³ |ì§€ë§Œ|ëŠ”ë°|ë•Œë¬¸ì—|ì´ì§€ë§Œ|ë©´ì„œë„|ë©´ì„œëŠ”|ê³ ì„œ)"

# #     @staticmethod
# #     def count_verbs(text):
# #         doc = nlp(text)
# #         return sum(1 for sentence in doc.sentences for word in sentence.words if word.upos == "VERB")

# #     @classmethod
# #     def split_by_connectives(cls, text):
# #         parts = re.split(cls.CONNECTIVE_PATTERN, text)
# #         results = []
# #         i = 0
# #         while i < len(parts):
# #             if re.match(cls.CONNECTIVE_PATTERN, parts[i]):
# #                 if i + 1 < len(parts):
# #                     results.append(parts[i + 1].strip())
# #                     i += 2
# #                 else:
# #                     i += 1
# #             else:
# #                 if parts[i].strip():
# #                     results.append(parts[i].strip())
# #                 i += 1
# #         return results

# #     @classmethod
# #     def split_if_needed(cls, text):
# #         return [text.strip()] if cls.count_verbs(text) <= 1 else cls.split_by_connectives(text)


# # class KoBERTSimilarity:
# #     def __init__(self, model_name="skt/kobert-base-v1"):
# #         self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# #         self.tokenizer = AutoTokenizer.from_pretrained(model_name)
# #         self.model = AutoModel.from_pretrained(model_name).to(self.device)
# #         self.model.eval()

# #     def get_embedding(self, text):
# #         inputs = self.tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=64).to(self.device)
# #         if "token_type_ids" in inputs:
# #             del inputs["token_type_ids"]
# #         with torch.no_grad():
# #             outputs = self.model(**inputs)
# #         return outputs.last_hidden_state[:, 0, :].squeeze(0)

# #     def similarity(self, text1, text2):
# #         emb1 = self.get_embedding(text1)
# #         emb2 = self.get_embedding(text2)
# #         return F.cosine_similarity(emb1, emb2, dim=0).item()

# #     def most_similar(self, target, candidates):
# #         best_score = -1
# #         best_text = ""
# #         for c in candidates:
# #             score = self.similarity(target, c)
# #             if score > best_score:
# #                 best_score = score
# #                 best_text = c
# #         return best_text, best_score


# # class Summarizer:
# #     def __init__(self, model_dir="lcw99/t5-base-korean-text-summary"):
# #         self.tokenizer = AutoTokenizer.from_pretrained(model_dir)
# #         self.model = AutoModelForSeq2SeqLM.from_pretrained(model_dir)
# #         self.max_input_length = 2048

# #     def summarize(self, text, max_length=128):
# #         inputs = self.tokenizer([text], max_length=self.max_input_length, truncation=True, return_tensors="pt", padding=True)
# #         output = self.model.generate(**inputs, num_beams=16, do_sample=False, min_length=1, max_length=max_length)
# #         decoded = self.tokenizer.batch_decode(output, skip_special_tokens=True)[0]
# #         return nltk.sent_tokenize(decoded.strip())[0]


# # class TopicExtractor:
# #     def __init__(self):
# #         self.summarizer = Summarizer()
# #         self.similarity = KoBERTSimilarity()

# #     def extract_topic(self, title, body):
# #         # 1. ìš”ì•½
# #         summary = self.summarizer.summarize(body)
# #         print(f"\nğŸ“Œ ìš”ì•½ ê²°ê³¼: {summary}")

# #         # 2. ë¶„ë¦¬
# #         candidates = SentenceSplitter.split_if_needed(summary)
# #         print("\nğŸ“Œ ë¶„ë¦¬ëœ ë¬¸ì¥ í›„ë³´:")
# #         for i, c in enumerate(candidates, 1):
# #             print(f"  [{i}] {c}")

# #         # 3. ìœ ì‚¬ë„
# #         best_sentence, score = self.similarity.most_similar(title, candidates)
# #         print(f"\nâœ… ìµœì¢… ì±„íƒ ë¬¸ì¥: {best_sentence} (ìœ ì‚¬ë„: {score:.4f})")
# #         return best_sentence


# # # ğŸ” ì˜ˆì‹œ ì‹¤í–‰
# # if __name__ == "__main__":
# #     title = "ê¹€ ì˜ì›, ì¥ì• ì¸ì˜ˆìˆ ë‹¨ ì„¤ë¦½ ì§ˆì˜"
# #     body = """
# #     ê¹€ ì˜ì›ì€ ì„¸ì¢…ì‹œêµìœ¡ì²­ ì–´ìš¸ë¦¼ì¥ì• ì¸ì˜ˆìˆ ë‹¨ì„ ì†Œê°œí–ˆê³ , ê¹€ êµìœ¡ê°ì—ê²Œ ì¥ì• ì¸ì˜ˆìˆ ë‹¨ ì„¤ë¦½ì— ëŒ€í•´ ì§ˆì˜í–ˆë‹¤.
# #     """

# #     extractor = TopicExtractor()
# #     topic = extractor.extract_topic(title, body)

# import re
# import torch
# import torch.nn.functional as F
# import stanza
# from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModel
# import nltk

# # NLTK & Stanza ë‹¤ìš´ë¡œë“œ
# nltk.download("punkt")
# stanza.download("ko")

# # Stanza ì´ˆê¸°í™”
# nlp = stanza.Pipeline(lang='ko', processors='tokenize,pos,lemma,depparse')


# class SentenceSplitter:
#     CONNECTIVE_PATTERN = r"(í•˜ë©´ì„œë„|ê·¸ëŸ¬ë©´ì„œ|ê·¸ëŸ¬ë‚˜|ê·¸ëŸ°ë°|í•˜ì§€ë§Œ|ê³ ì„œ|ê³ ì|ë ¤ê³ |ë¼ê³ ë„|ë¼ë©°|í–ˆê³ |í•˜ë©°|í–ˆë‹¤ë©°|ë©´ì„œ|ì´ì§€ë§Œ|ì¸ë°)"
#     CONJUNCTION_LEMMAS = {"ê·¸ëŸ¬ë‚˜", "í•˜ì§€ë§Œ", "ê²Œë‹¤ê°€", "ë˜í•œ", "ê·¸ë¦¬ê³ ", "ê·¸ëŸ¬ë©´ì„œ"}
#     CCONJ_RELATIONS = {"conj", "cc", "advcl"}

#     @staticmethod
#     def count_verbs(text):
#         doc = nlp(text)
#         return sum(1 for sentence in doc.sentences for word in sentence.words if word.upos == "VERB")

#     @classmethod
#     def split_by_stanza(cls, text):
#         doc = nlp(text)
#         results = []

#         for sent in doc.sentences:
#             split_points = []

#             # 1. ì—°ê²°ì–´ ê¸°ë°˜ split
#             for match in re.finditer(cls.CONNECTIVE_PATTERN, sent.text):
#                 split_points.append(match.end())

#             # 2. ì ‘ì†ë¶€ì‚¬ ê¸°ë°˜ split
#             for word in sent.words:
#                 if word.upos == "ADV" and word.lemma in cls.CONJUNCTION_LEMMAS:
#                     split_points.append(word.start_char)

#             # 3. ì˜ì¡´êµ¬ë¬¸ ê¸°ë°˜ split
#             for word in sent.words:
#                 if word.deprel in cls.CCONJ_RELATIONS:
#                     split_points.append(word.start_char)

#             # ì •ë¦¬
#             split_points = sorted(set(split_points))
#             if not split_points:
#                 results.append(sent.text.strip())
#             else:
#                 prev = 0
#                 for idx in split_points:
#                     chunk = sent.text[prev:idx].strip()
#                     if chunk:
#                         results.append(chunk)
#                     prev = idx
#                 last_chunk = sent.text[prev:].strip()
#                 if last_chunk:
#                     results.append(last_chunk)

#         return results

#     @classmethod
#     def split_if_needed(cls, text):
#         verb_count = cls.count_verbs(text)
#         return [text.strip()] if verb_count <= 1 else cls.split_by_stanza(text)


# class KoBERTSimilarity:
#     def __init__(self, model_name="skt/kobert-base-v1"):
#         self.device = torch.device(
#             "cuda" if torch.cuda.is_available() else "cpu")
#         self.tokenizer = AutoTokenizer.from_pretrained(model_name)
#         self.model = AutoModel.from_pretrained(model_name).to(self.device)
#         self.model.eval()

#     def get_embedding(self, text):
#         inputs = self.tokenizer(text, return_tensors="pt", truncation=True,
#                                 padding=True, max_length=64).to(self.device)
#         if "token_type_ids" in inputs:
#             del inputs["token_type_ids"]
#         with torch.no_grad():
#             outputs = self.model(**inputs)
#         return outputs.last_hidden_state[:, 0, :].squeeze(0)

#     def similarity(self, text1, text2):
#         emb1 = self.get_embedding(text1)
#         emb2 = self.get_embedding(text2)
#         return F.cosine_similarity(emb1, emb2, dim=0).item()

#     def most_similar(self, target, candidates):
#         best_score = -1
#         best_text = ""
#         for c in candidates:
#             score = self.similarity(target, c)
#             if score > best_score:
#                 best_score = score
#                 best_text = c
#         return best_text, best_score


# class Summarizer:
#     def __init__(self, model_dir="lcw99/t5-base-korean-text-summary"):
#         self.tokenizer = AutoTokenizer.from_pretrained(model_dir)
#         self.model = AutoModelForSeq2SeqLM.from_pretrained(model_dir)
#         self.max_input_length = 2048

#     def summarize(self, text, max_length=128):
#         inputs = self.tokenizer([text], max_length=self.max_input_length,
#                                 truncation=True, return_tensors="pt", padding=True)
#         output = self.model.generate(
#             **inputs, num_beams=16, do_sample=False, min_length=1, max_length=max_length)
#         decoded = self.tokenizer.batch_decode(
#             output, skip_special_tokens=True)[0]
#         return nltk.sent_tokenize(decoded.strip())[0]


# class TopicExtractor:
#     def __init__(self):
#         self.summarizer = Summarizer()
#         self.similarity = KoBERTSimilarity()

#     def extract_topic(self, title, body):
#         # 1. ìš”ì•½
#         summary = self.summarizer.summarize(body)
#         print(f"\nğŸ“Œ ìš”ì•½ ê²°ê³¼: {summary}")

#         # 2. ë¶„ë¦¬
#         candidates = SentenceSplitter.split_if_needed(summary)
#         print("\nğŸ“Œ ë¶„ë¦¬ëœ ë¬¸ì¥ í›„ë³´:")
#         for i, c in enumerate(candidates, 1):
#             print(f"  [{i}] {c}")

#         # 3. ìœ ì‚¬ë„ ê¸°ë°˜ ìµœì¢… ì„ íƒ
#         best_sentence, score = self.similarity.most_similar(title, candidates)
#         print(f"\nâœ… ìµœì¢… ì±„íƒ ë¬¸ì¥: {best_sentence} (ìœ ì‚¬ë„: {score:.4f})")
#         return best_sentence


# # ğŸ” ì˜ˆì‹œ ì‹¤í–‰
# if __name__ == "__main__":
#     title = "ê¹€ ì˜ì›, ì¥ì• ì¸ì˜ˆìˆ ë‹¨ ì„¤ë¦½ ì§ˆì˜"
#     body = """
#     ê¹€ ì˜ì›ì€ ì„¸ì¢…ì‹œêµìœ¡ì²­ ì–´ìš¸ë¦¼ì¥ì• ì¸ì˜ˆìˆ ë‹¨ì„ ì†Œê°œí–ˆê³ , ê¹€ êµìœ¡ê°ì—ê²Œ ì¥ì• ì¸ì˜ˆìˆ ë‹¨ ì„¤ë¦½ì— ëŒ€í•´ ì§ˆì˜í–ˆë‹¤.
#     """

#     extractor = TopicExtractor()
#     topic = extractor.extract_topic(title, body)

# import re
# import torch
# import torch.nn.functional as F
# import stanza
# from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModel
# import nltk

# # NLTK & Stanza ë‹¤ìš´ë¡œë“œ
# nltk.download("punkt")
# stanza.download("ko")

# # Stanza ì´ˆê¸°í™”
# nlp = stanza.Pipeline(lang='ko', processors='tokenize,pos,lemma,depparse')


# class SentenceSplitter:
#     CONNECTIVE_PATTERN = r"(ë©´ì„œ|ë©°|í•˜ë©°|ë¼ë©°|í•˜ê³ |ì§€ë§Œ|ëŠ”ë°|ë•Œë¬¸ì—|ì´ì§€ë§Œ|ë©´ì„œë„|ë©´ì„œëŠ”|ê³ ì„œ)"
#     DEP_LABELS = {"conj", "advcl", "parataxis"}

#     @staticmethod
#     def count_verbs(text):
#         doc = nlp(text)
#         return sum(1 for sentence in doc.sentences for word in sentence.words if word.upos == "VERB")

#     @classmethod
#     def split_by_connectives(cls, text):
#         parts = re.split(cls.CONNECTIVE_PATTERN, text)
#         results = []
#         i = 0
#         while i < len(parts):
#             if re.match(cls.CONNECTIVE_PATTERN, parts[i]):
#                 if i + 1 < len(parts):
#                     results.append(parts[i + 1].strip())
#                     i += 2
#                 else:
#                     i += 1
#             else:
#                 if parts[i].strip():
#                     results.append(parts[i].strip())
#                 i += 1
#         return results

#     @classmethod
#     def split_by_dependency(cls, text):
#         doc = nlp(text)
#         for sent in doc.sentences:
#             words = sent.words
#             spans = []

#             for word in words:
#                 if word.deprel in cls.DEP_LABELS:
#                     spans.append(word.start_char)

#             if not spans:
#                 return [text.strip()]

#             spans = sorted(set(spans))
#             spans.append(len(text))
#             prev = 0
#             return [text[prev:sp].strip() for sp in spans if text[prev:sp].strip() and not (prev := sp)]

#         return [text.strip()]

#     @classmethod
#     def split_if_needed(cls, text):
#         verb_count = cls.count_verbs(text)
#         if verb_count <= 1:
#             return [text.strip()]
#         split_by_dep = cls.split_by_dependency(text)
#         if len(split_by_dep) > 1:
#             return split_by_dep
#         return cls.split_by_connectives(text)


# class KoBERTSimilarity:
#     def __init__(self, model_name="skt/kobert-base-v1"):
#         self.device = torch.device(
#             "cuda" if torch.cuda.is_available() else "cpu")
#         self.tokenizer = AutoTokenizer.from_pretrained(model_name)
#         self.model = AutoModel.from_pretrained(model_name).to(self.device)
#         self.model.eval()

#     def get_embedding(self, text):
#         inputs = self.tokenizer(text, return_tensors="pt", truncation=True,
#                                 padding=True, max_length=64).to(self.device)
#         if "token_type_ids" in inputs:
#             del inputs["token_type_ids"]
#         with torch.no_grad():
#             outputs = self.model(**inputs)
#         return outputs.last_hidden_state[:, 0, :].squeeze(0)

#     def similarity(self, text1, text2):
#         emb1 = self.get_embedding(text1)
#         emb2 = self.get_embedding(text2)
#         return F.cosine_similarity(emb1, emb2, dim=0).item()

#     def most_similar(self, target, candidates):
#         best_score = -1
#         best_text = ""
#         if len(candidates) == 1:
#             return candidates[0], 1
#         for c in candidates:
#             score = self.similarity(target, c)
#             if score > best_score:
#                 best_score = score
#                 best_text = c
#         return best_text, best_score


# class Summarizer:
#     def __init__(self, model_dir="lcw99/t5-base-korean-text-summary"):
#         self.tokenizer = AutoTokenizer.from_pretrained(model_dir)
#         self.model = AutoModelForSeq2SeqLM.from_pretrained(model_dir)
#         self.max_input_length = 2048

#     def summarize(self, text, max_length=128):
#         inputs = self.tokenizer([text], max_length=self.max_input_length,
#                                 truncation=True, return_tensors="pt", padding=True)
#         output = self.model.generate(
#             **inputs, num_beams=16, do_sample=False, min_length=1, max_length=max_length)
#         decoded = self.tokenizer.batch_decode(
#             output, skip_special_tokens=True)[0]
#         return nltk.sent_tokenize(decoded.strip())[0]


# class TopicExtractor:
#     def __init__(self):
#         self.summarizer = Summarizer()
#         self.similarity = KoBERTSimilarity()

#     def extract_topic(self, title, body):
#         summary = self.summarizer.summarize(body)
#         print(f"\nğŸ“Œ ìš”ì•½ ê²°ê³¼: {summary}")

#         candidates = SentenceSplitter.split_if_needed(summary)
#         print("\nğŸ“Œ ë¶„ë¦¬ëœ ë¬¸ì¥ í›„ë³´:")
#         for i, c in enumerate(candidates, 1):
#             print(f"  [{i}] {c}")

#         best_sentence, score = self.similarity.most_similar(title, candidates)
#         print(f"\nâœ… ìµœì¢… ì±„íƒ ë¬¸ì¥: {best_sentence} (ìœ ì‚¬ë„: {score:.4f})")
#         return best_sentence


# # ğŸ” ì˜ˆì‹œ ì‹¤í–‰
# if __name__ == "__main__":
#     title = "ê¹€ ì˜ì›, ì¥ì• ì¸ì˜ˆìˆ ë‹¨ ì„¤ë¦½ ì§ˆì˜"
#     body = """
#     ê¹€ ì˜ì›ì€ ì„¸ì¢…ì‹œêµìœ¡ì²­ ì–´ìš¸ë¦¼ì¥ì• ì¸ì˜ˆìˆ ë‹¨ì„ ì†Œê°œí–ˆê³ , ê¹€ êµìœ¡ê°ì—ê²Œ ì¥ì• ì¸ì˜ˆìˆ ë‹¨ ì„¤ë¦½ì— ëŒ€í•´ ì§ˆì˜í–ˆë‹¤.
#     """
#     extractor = TopicExtractor()
#     topic = extractor.extract_topic(title, body)

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import nltk
import stanza

# NLTK, Stanza ì´ˆê¸°í™”
nltk.download('punkt')
stanza.download('ko')
nlp = stanza.Pipeline(lang='ko', processors='tokenize,pos,lemma,depparse')


class Summarizer:
    def __init__(self, model_dir="lcw99/t5-base-korean-text-summary"):
        self.tokenizer = AutoTokenizer.from_pretrained(model_dir)
        self.model = AutoModelForSeq2SeqLM.from_pretrained(model_dir)
        self.max_input_length = 2048

    def summarize(self, text, max_length=128):
        inputs = self.tokenizer([text], max_length=self.max_input_length,
                                truncation=True, return_tensors="pt", padding=True)
        output = self.model.generate(
            **inputs, num_beams=16, do_sample=False, min_length=1, max_length=max_length)
        decoded = self.tokenizer.batch_decode(
            output, skip_special_tokens=True)[0]
        return nltk.sent_tokenize(decoded.strip())[0]


class SentenceCleaner:
    def __init__(self):
        self.valid_deprels = {"root", "nsubj", "obj", "iobj", "obl", "ccomp", "conj"}
        self.valid_upos = {"NOUN", "PROPN", "VERB", "PRON"}
        self.valid_xpos = {"ncn", "nq", "ncpa", "xsv", "ep", "ef", "jp", "px", "etm"}

    def is_valid_word(self, word):
        # í˜•íƒœì†Œ ë¶„í• 
        xpos_parts = word.xpos.split("+")
        
        # ì¡°ê±´ ì²´í¬
        has_valid_deprel = word.deprel in self.valid_deprels
        has_valid_upos = word.upos in self.valid_upos
        has_valid_xpos = any(x in self.valid_xpos for x in xpos_parts)

        # êµì°¨ ì¡°ê±´ 2ê°œ ì´ìƒ ë§Œì¡± ì‹œ í¬í•¨
        return sum([has_valid_deprel, has_valid_upos, has_valid_xpos]) >= 2

    def clean_summary(self, summary: str) -> str:
        doc = nlp(summary)

        for sentence in doc.sentences:
            important_words = []
            for word in sentence.words:
                print(f"{word.text}\t{word.deprel}\t{word.upos}\t{word.xpos}")
                
                if self.is_valid_word(word):
                    important_words.append(word.text)

            if important_words:
                return " ".join(important_words)

        return summary  # fallback


class TopicExtractor:
    def __init__(self):
        self.summarizer = Summarizer()
        self.cleaner = SentenceCleaner()

    def extract_topic(self, title, body):
        summary = self.summarizer.summarize(body)
        print(f"\nğŸ“Œ ìš”ì•½ ê²°ê³¼: {summary}")

        cleaned = self.cleaner.clean_summary(summary)
        print(f"âœ… ìˆ˜ì‹ì–´ ì œê±° í›„ í•µì‹¬ ë¬¸ì¥: {cleaned}")
        return cleaned


# ğŸ” ì˜ˆì‹œ ì‹¤í–‰
if __name__ == "__main__":
    title = "ê¹€ ì˜ì›, ì¥ì• ì¸ì˜ˆìˆ ë‹¨ ì„¤ë¦½ ì§ˆì˜"
    body1 = """
    ì´ë²ˆì— ì˜ê²°ëœ ê³µì‚¬ë²• ì¼ë¶€ ê°œì •ì•ˆì€ ë†ì–´ì´Œê³µì‚¬ê°€ í•´ì™¸ì—ì„œ ì°¸ì—¬í•  ìˆ˜ ìˆëŠ” ì‚¬ì—…ì˜ ì¢…ë¥˜ì™€ ë²”ìœ„ë¥¼ í™•ëŒ€í•˜ëŠ” ê²ƒì´ ì£¼ìš” ê³¨ìë‹¤. ê³µì‚¬ëŠ” ê·¸ë™ì•ˆ ë²•ì ì¸ ì œì•½ìœ¼ë¡œ â€˜í•´ì™¸ë†ì—…ê°œë°œ ë° ê¸°ìˆ ìš©ì—­ì‚¬ì—…â€™ì—ë§Œ ì°¸ì—¬í•  ìˆ˜ ìˆì—ˆë‹¤. í•˜ì§€ë§Œ ì´ë²ˆì— ë²•ì´ ê°œì •ë˜ë©´ì„œ ë†ì‚°ì—…ë‹¨ì§€ì™€ ì§€ì—­ê°œë°œ, ë†ì–´ì´Œìš©ìˆ˜ ë° ì§€í•˜ìˆ˜ìì› ê°œë°œ ë“± ë³´ë‹¤ ê´‘ë²”ìœ„í•œ ë¶„ì•¼ì˜ í•´ì™¸ì‚¬ì—… ì°¸ì—¬ê°€ ê°€ëŠ¥í•´ì¡Œë‹¤.
ê³µì‚¬ëŠ” ì´ë²ˆ ë²•ë¥ ê°œì •ì— ë”°ë¼ ê·¸ë™ì•ˆ í•´ì™¸ì‚¬ì—…ì„ í†µí•´ ì¶•ì í•œ ê²½í—˜ê³¼ ì¸ì  ë„¤íŠ¸ì›Œí¬ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë¯¼ê°„ê¸°ì—… ë“±ê³¼ ì—°ê³„í•´ ê°œë„êµ­ ë†ì´Œê°œë°œì— ì ê·¹ ë‚˜ì„ ë‹¤ëŠ” ê³„íšì´ë‹¤.ì´ë²ˆ ê°œì •ì•ˆì„ ëŒ€í‘œ ë°œì˜í•œ ê¹€í˜„ê¶Œ ë”ë¶ˆì–´ë¯¼ì£¼ë‹¹ ì˜ì›ì€ "ê°œë°œë„ìƒêµ­ì„ ì¤‘ì‹¬ìœ¼ë¡œ ë§ì€ êµ­ê°€ê°€ êµ­ë‚´ì˜ ì•ì„  ë†ì‚°ì—… ê¸°ìˆ ê³¼ ë…¸í•˜ìš°ë¥¼ ì „ìˆ˜ë°›ê¸¸ ì›í•˜ê³  ìˆê³  êµ­ë‚´ ë¯¼ê°„ê¸°ì—…ë“¤ë„ í•´ì™¸ì§„ì¶œì— ë²•ì ì¸ ì¥ë²½ì´ ìˆì–´ ì–´ë ¤ì›€ì´ ë§ì•˜ëŠ”ë° ì´ë¥¼ í•´ì†Œí•´ ì£¼ëŠ” ê²ƒì´ êµ­íšŒì˜ ì—­í• "ì´ë¼ë©° "ì´ë²ˆ ë²• ê°œì •ìœ¼ë¡œ í•œêµ­ë†ì–´ì´Œê³µì‚¬ê°€ ë¯¼ê°„ê¸°ì—…ê³¼ í•¨ê»˜ í•´ì™¸ì— ì§„ì¶œí•  ìˆ˜ ìˆëŠ” ê¸¸ì´ ì—´ë ¸ê¸° ë•Œë¬¸ì— ì•ìœ¼ë¡œ êµ­ë‚´ ë†ì‚°ì—… ì‹œì¥ì˜ ìƒˆë¡œìš´ í™œë¡œë¥¼ ëª¨ìƒ‰í•˜ê³  ì¼ìì¹˜ ì°½ì¶œì— ê¸°ì—¬í•  ìˆ˜ ìˆì„ ê²ƒìœ¼ë¡œ ê¸°ëŒ€í•œë‹¤"ê³  ë§í–ˆë‹¤.
    """
    body2 = """
    ê¹€í˜„ê¶Œ êµ­íšŒì˜ì›(ë”ë¶ˆì–´ë¯¼ì£¼ë‹¹Â·ë¹„ë¡€ëŒ€í‘œÂ·ì‚¬ì§„)ì€ "ìµœê·¼ êµ­ë°©ë¶€ì˜ í†µí•©ì‹ ê³µí•­ ë¶€ì§€ì„ ì • ë°œí‘œë¥¼ í™˜ì˜í•œë‹¤"ë©´ì„œ "ì•ìœ¼ë¡œ êµ¬ë¯¸ì‹œë¥¼ ì‹ ê³µí•­ ë°°í›„ êµí†µÂ·ë¬¼ë¥˜Â·ì‚°ì—…ì˜ ì¤‘ì‹¬ì§€ë¡œ ì»¤ ë‚˜ê°€ë„ë¡ ì§€ì›ì„ ì•„ë¼ì§€ ì•Šê² ë‹¤"ê³  30ì¼ ë°í˜”ë‹¤.
    """
    body3 = """
    ì£¼ë³€ ë„ì‹œë¥¼ ì‡ëŠ” êµí†µë§ í™•ì¶© ì—­ì‹œ ì‹ ê³µí•­ì˜ ì„±íŒ¨ë¥¼ ì¢Œìš°í•  í•µì‹¬ê³¼ì œë¡œ ë– ì˜¤ë¥´ê³  ìˆë‹¤. ê²½ë¶ë„ì— ë”°ë¥´ë©´ 2021ë…„ë¶€í„° ì „ì²  4ê³³, ê³ ì†ë„ë¡œ 2ê³³ ë“± ì´ 260ãì— ê±¸ì³ êµ­ë¹„ 6ì¡°ì›ì„ íˆ¬ì…í•˜ëŠ” ì‹ ê³µí•­ê³¼ êµ¬ë¯¸Â·í¬í•­Â·ëŒ€êµ¬ ë“± ì¸ê·¼ ë„ì‹œë“¤ì„ ì—°ê²°í•˜ëŠ” êµí†µë§ í™•ì¶©ì‚¬ì—…ì´ ì¶”ì§„ëœë‹¤.
ê¹€ ì˜ì›ì€ "êµ¬ë¯¸ì‹œê°€ ì‹ ê³µí•­ë°°í›„ë‹¨ì§€ë¡œì„œ ì‚°ì—…Â·êµí†µÂ·ë¬¼ë¥˜ì˜ ì¤‘ì‹¬ì§€ë¡œ ë¶€ìƒí•˜ë©´ êµ¬ë¯¸ì‚°ë‹¨ì´ë‚˜ ì•„íŒŒíŠ¸ ì‹ ë„ì‹œ í™œì„±í™”ë¿ë§Œ ì•„ë‹ˆë¼ ë„ì‹œì™€ ë†ì´Œì´ ì¡°í™”í•˜ëŠ” ì§€ì—­ ê· í˜•ë°œì „ì´ ì´ë¤„ì§ˆ ê²ƒ"ì´ë¼ê³  ë‚´ë‹¤ë´¤ë‹¤.
    """
    extractor = TopicExtractor()
    topic = extractor.extract_topic(title, body1)
    topic = extractor.extract_topic(title, body2)
    topic = extractor.extract_topic(title, body3)
