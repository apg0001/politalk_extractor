from transformers import BartForConditionalGeneration
import torch
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline
import nltk
import stanza

# NLTK, Stanza ì´ˆê¸°í™”
nltk.download('punkt')
stanza.download('ko')
nlp = stanza.Pipeline(lang='ko', processors='tokenize,pos,lemma,depparse')


class Summarizer:
    def __init__(self, model_dir="lcw99/t5-base-korean-text-summary"):
        self.tokenizer = AutoTokenizer.from_pretrained(model_dir)
        self.model = AutoModelForSeq2SeqLM.from_pretrained(model_dir)
        self.max_input_length = 2048

    def summarize(self, text, max_length=128):
        inputs = self.tokenizer([text], max_length=self.max_input_length,
                                truncation=True, return_tensors="pt", padding=True)
        output = self.model.generate(
            **inputs, num_beams=16, do_sample=False, min_length=1, max_length=max_length)
        decoded = self.tokenizer.batch_decode(
            output, skip_special_tokens=True)[0]
        return nltk.sent_tokenize(decoded.strip())[0]


# class SentenceCleaner:
#     def __init__(self):
#         self.valid_deprels = {"root", "nsubj",
#                               "obj", "iobj", "obl", "ccomp", "conj", "xcomp", "advcl", "acl", "nmod", "compound", "conj", "dislocated", "dep", "aux"}
#         self.valid_upos = {"NOUN", "PROPN", "VERB", "PRON", "AUX", "CCONJ", "SCONJ", "CONJ", "ADV", "ADJ"}
#         self.valid_xpos = {}  # {"ncn", "nq", "ncpa", "xsv", "ep", "ef", "jp", "px", "etm"}

#     def is_valid_word(self, word):
#         # í˜•íƒœì†Œ ë¶„í• 
#         xpos_parts = word.xpos.split("+")

#         # ì¡°ê±´ ì²´í¬
#         has_valid_deprel = word.deprel in self.valid_deprels
#         has_valid_upos = word.upos in self.valid_upos
#         has_valid_xpos = any(x in self.valid_xpos for x in xpos_parts)

#         # êµì°¨ ì¡°ê±´ 2ê°œ ì´ìƒ ë§Œì¡± ì‹œ í¬í•¨
#         return sum([has_valid_deprel, has_valid_upos, has_valid_xpos]) >= 2

#     def clean_summary(self, summary: str) -> list:
#         doc = nlp(summary)

#         for sentence in doc.sentences:
#             important_words = []
#             for word in sentence.words:
#                 # print(f"{word.text}\t{word.deprel}\t{word.upos}\t{word.xpos}")

#                 if self.is_valid_word(word):
#                     important_words.append(word.text)
#                     if((word.upos in {"CONJ", "SCONJ", "CCONJ"} and word.deprel in {"conj", "root", "obl", "advcl", "cc"})):
#                     #    or \
#                         # (word.upos == "AUX" and word.deprel == "aux" and word[word.id].upos not in {"CONJ", "SCONJ", "CCONJ", "AUX"} and word.text not in {"ìˆëŠ”"})):
#                         break

#             if important_words:
#                 return " ".join(important_words)

#         return summary  # fallback
class SentenceCleaner:
    def __init__(self):
        self.valid_deprels = {
            "root", "nsubj", "obj", "iobj", "obl", "ccomp", "conj", "xcomp",
            "advcl", "acl", "nmod", "compound", "dislocated", "dep", "aux"
        }
        self.valid_upos = {
            "NOUN", "PROPN", "VERB", "PRON", "AUX", "CCONJ", "SCONJ", "CONJ", "ADV", "ADJ"
        }
        self.valid_xpos = set()  # ë¹„ì›Œë‘ . ì¡°ê±´ 2ê°œ ì´ìƒì´ë©´ í†µê³¼ë¨.

        self.connecting_deprels = {"conj", "root", "obl", "advcl", "cc"}
        self.connecting_upos = {"CONJ", "SCONJ", "CCONJ"}

    def is_valid_word(self, word):
        xpos_parts = word.xpos.split("+")
        has_valid_deprel = word.deprel in self.valid_deprels
        has_valid_upos = word.upos in self.valid_upos
        has_valid_xpos = any(x in self.valid_xpos for x in xpos_parts)
        return sum([has_valid_deprel, has_valid_upos, has_valid_xpos]) >= 2

    def clean_summary(self, summary: str) -> list[str]:
        doc = nlp(summary)

        for sentence in doc.sentences:
            front = []
            back = []
            switching = False

            for word in sentence.words:
                if self.is_valid_word(word):
                    if (word.upos in self.connecting_upos and word.deprel in self.connecting_deprels):
                        switching = True
                        continue  # ì—°ê²°ì–´ëŠ” ì œì™¸

                    if switching:
                        back.append(word.text)
                    else:
                        front.append(word.text)

            return [" ".join(front).strip(), " ".join(back).strip()]

        return [summary.strip(), ""]  # fallback: ë‚˜ëˆ„ì§€ ëª»í•œ ê²½ìš°


class SummarySelector:
    def __init__(self):
        stanza.download("ko", processors="tokenize,pos,lemma", verbose=False)
        self.nlp = stanza.Pipeline(
            "ko", processors="tokenize,pos,lemma", verbose=False)

    def get_lemmas(self, text):
        """ë¬¸ì¥ì—ì„œ lemma(í‘œì œì–´) ì§‘í•© ì¶”ì¶œ"""
        doc = self.nlp(text)
        lemmas = set()
        for sentence in doc.sentences:
            for word in sentence.words:
                lemmas.add(word.lemma)
        return lemmas

    def jaccard_similarity(self, lemmas1, lemmas2):
        """ë‘ ì§‘í•© ê°„ ìì¹´ë“œ ìœ ì‚¬ë„ ê³„ì‚°"""
        intersection = lemmas1 & lemmas2
        union = lemmas1 | lemmas2
        if not union:
            return 0.0
        return len(intersection) / len(union)

    def select_least_similar(self, summary_list, purpose, speech):
        """ê°€ì¥ ëœ ìœ ì‚¬í•œ ìš”ì•½ë¬¸ ì„ íƒ (í‰ê·  ìœ ì‚¬ë„ 50% ë„˜ìœ¼ë©´ ë¹ˆì¹¸ ë°˜í™˜)"""
        purpose_lemmas = self.get_lemmas(purpose)
        speech_lemmas = self.get_lemmas(speech)

        min_similarity = float("inf")
        selected_summary = None

        for summary in summary_list:
            summary_lemmas = self.get_lemmas(summary)
            sim_purpose = self.jaccard_similarity(
                summary_lemmas, purpose_lemmas)
            sim_speech = self.jaccard_similarity(summary_lemmas, speech_lemmas)
            avg_similarity = (sim_purpose + sim_speech) / 2

            if avg_similarity < min_similarity:
                min_similarity = avg_similarity
                selected_summary = summary

        # í‰ê·  ìœ ì‚¬ë„ê°€ 0.5 ì´ìƒì´ë©´ ë¹ˆì¹¸ ë°˜í™˜
        if min_similarity >= 0.5:
            return ""
        if min_similarity == avg_similarity:
            if summary_list[1] != "":
                return summary_list[1]

        return selected_summary

# class Paraphraser:
#     # ëª¨ë¸ ì´ë¦„ ì„¤ì •
#     model_name = "psyche/KoT5-paraphrase-generation"

#     # pipelineì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¶ˆëŸ¬ì˜¤ê¸°
#     generator = pipeline("text2text-generation",
#                          model=model_name, device=0)  # device=0ì€ GPU ì‚¬ìš© ì‹œ

#     @classmethod
#     def generate(cls, prompt, max_tokens=128):
#         # í…ìŠ¤íŠ¸ ìƒì„±
#         # result = cls.generator(prompt, max_length=512, num_return_sequences=1, max_new_tokens=max_tokens)
#         result = cls.generator(prompt, max_length=512, num_return_sequences=1)
#         # print(result)

#         # ìƒì„±ëœ í…ìŠ¤íŠ¸ ë°˜í™˜
#         return result[0]['generated_text']


class TopicExtractor:
    def __init__(self):
        self.summarizer = Summarizer()
        self.cleaner = SentenceCleaner()
        # self.paraphraser = Paraphraser()
        self.selector = SummarySelector()
        # self.paraphraser2 = Paraphrase()

    def extract_topic(self, title, body, purpose, sentence):
        summary = self.summarizer.summarize(body)
        print(f"\nìš”ì•½ ê²°ê³¼:\t{summary}")

        cleaned = self.cleaner.clean_summary(summary)
        print(f"ìˆ˜ì‹ì–´ ì œê±°:\t{cleaned}")

        selected = self.selector.select_least_similar(
            cleaned, purpose, sentence)

        # paraphrased = self.paraphraser.generate(cleaned)
        # print(f"paraphrase:\t{paraphrased}")

        return selected


# ğŸ” ì˜ˆì‹œ ì‹¤í–‰
if __name__ == "__main__":
    title = "ê¹€ ì˜ì›, ì¥ì• ì¸ì˜ˆìˆ ë‹¨ ì„¤ë¦½ ì§ˆì˜"
    body1 = """
    í†µí•©ë‹¹ ê°„ì‚¬ì¸ ì´ì±„ìµ ì˜ì›ì€ ì½”ë¡œë‚˜19 ìê°€ê²©ë¦¬ìì—ê²Œ ê±°ì†Œíˆ¬í‘œÂ·ì„ ìƒíˆ¬í‘œë¥¼ í—ˆìš©í•˜ëŠ” ë“± ëŒ€ì±… ë§ˆë ¨ì„ ì£¼ë¬¸í–ˆê³ , ì´ìŠ¹íƒ í›„ë³´ìëŠ” "ë³´ê±´ë‹¹êµ­ì˜ ì´ë™ì œí•œ í—ˆìš©ì„ ì „ì œë¡œ ì‚¬ì „íˆ¬í‘œê°€ ê°€ëŠ¥í•  ê²ƒ ê°™ë‹¤"ë©´ì„œ "ì°¸ì •ê¶Œ í™•ëŒ€ë¼ëŠ” ë¶€ë¶„ê³¼ ê´€ë ¨í•´ì„œ ì ê·¹ ì˜ê²¬ì„ ê°œì§„í•˜ê² ë‹¤"ê³  ë‹µí–ˆë‹¤.
ë¯¼ì£¼ë‹¹ ê¶Œë¯¸í˜ ì˜ì›ì€ "ì „ìê±°ì†Œíˆ¬í‘œ ë„ì…ì„ ê²€í† í•´ì•¼ í•œë‹¤"ë©° ì„ ê´€ìœ„ì˜ ì˜¨ë¼ì¸íˆ¬í‘œì‹œìŠ¤í…œì¸ 'ì¼€ì´ë³´íŒ…'(K-voting) ì´ìš© ë°©ì•ˆì„ ì œì•ˆí–ˆë‹¤.
    """
#     body2 = """
#     ê¹€í˜„ê¶Œ êµ­íšŒì˜ì›(ë”ë¶ˆì–´ë¯¼ì£¼ë‹¹Â·ë¹„ë¡€ëŒ€í‘œÂ·ì‚¬ì§„)ì€ "ìµœê·¼ êµ­ë°©ë¶€ì˜ í†µí•©ì‹ ê³µí•­ ë¶€ì§€ì„ ì • ë°œí‘œë¥¼ í™˜ì˜í•œë‹¤"ë©´ì„œ "ì•ìœ¼ë¡œ êµ¬ë¯¸ì‹œë¥¼ ì‹ ê³µí•­ ë°°í›„ êµí†µÂ·ë¬¼ë¥˜Â·ì‚°ì—…ì˜ ì¤‘ì‹¬ì§€ë¡œ ì»¤ ë‚˜ê°€ë„ë¡ ì§€ì›ì„ ì•„ë¼ì§€ ì•Šê² ë‹¤"ê³  30ì¼ ë°í˜”ë‹¤.
#     """
#     body3 = """
#     ì£¼ë³€ ë„ì‹œë¥¼ ì‡ëŠ” êµí†µë§ í™•ì¶© ì—­ì‹œ ì‹ ê³µí•­ì˜ ì„±íŒ¨ë¥¼ ì¢Œìš°í•  í•µì‹¬ê³¼ì œë¡œ ë– ì˜¤ë¥´ê³  ìˆë‹¤. ê²½ë¶ë„ì— ë”°ë¥´ë©´ 2021ë…„ë¶€í„° ì „ì²  4ê³³, ê³ ì†ë„ë¡œ 2ê³³ ë“± ì´ 260ãì— ê±¸ì³ êµ­ë¹„ 6ì¡°ì›ì„ íˆ¬ì…í•˜ëŠ” ì‹ ê³µí•­ê³¼ êµ¬ë¯¸Â·í¬í•­Â·ëŒ€êµ¬ ë“± ì¸ê·¼ ë„ì‹œë“¤ì„ ì—°ê²°í•˜ëŠ” êµí†µë§ í™•ì¶©ì‚¬ì—…ì´ ì¶”ì§„ëœë‹¤.
# ê¹€ ì˜ì›ì€ "êµ¬ë¯¸ì‹œê°€ ì‹ ê³µí•­ë°°í›„ë‹¨ì§€ë¡œì„œ ì‚°ì—…Â·êµí†µÂ·ë¬¼ë¥˜ì˜ ì¤‘ì‹¬ì§€ë¡œ ë¶€ìƒí•˜ë©´ êµ¬ë¯¸ì‚°ë‹¨ì´ë‚˜ ì•„íŒŒíŠ¸ ì‹ ë„ì‹œ í™œì„±í™”ë¿ë§Œ ì•„ë‹ˆë¼ ë„ì‹œì™€ ë†ì´Œì´ ì¡°í™”í•˜ëŠ” ì§€ì—­ ê· í˜•ë°œì „ì´ ì´ë¤„ì§ˆ ê²ƒ"ì´ë¼ê³  ë‚´ë‹¤ë´¤ë‹¤.
#     """
    extractor = TopicExtractor()
    topic = extractor.extract_topic(title, body1)
    # topic = extractor.extract_topic(title, body2)
    # topic = extractor.extract_topic(title, body3)
