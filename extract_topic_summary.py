from transformers import BartForConditionalGeneration
import torch
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline
import nltk
import stanza
import re

# NLTK, Stanza ì´ˆê¸°í™”
nltk.download('punkt')
stanza.download('ko')
nlp = stanza.Pipeline(lang='ko', processors='tokenize,pos,lemma,depparse')


class Summarizer:
    def __init__(self, model_dir="lcw99/t5-base-korean-text-summary"):
        self.tokenizer = AutoTokenizer.from_pretrained(model_dir)
        self.model = AutoModelForSeq2SeqLM.from_pretrained(model_dir)
        self.max_input_length = 2048

    def summarize(self, text, max_length=128):
        inputs = self.tokenizer([text], max_length=self.max_input_length,
                                truncation=True, return_tensors="pt", padding=True)
        output = self.model.generate(
            **inputs, num_beams=16, do_sample=False, min_length=1, max_length=max_length)
        decoded = self.tokenizer.batch_decode(
            output, skip_special_tokens=True)[0]
        return nltk.sent_tokenize(decoded.strip())[0]


# class SentenceCleaner:
#     def __init__(self):
#         self.valid_deprels = {"root", "nsubj",
#                               "obj", "iobj", "obl", "ccomp", "conj", "xcomp", "advcl", "acl", "nmod", "compound", "conj", "dislocated", "dep", "aux"}
#         self.valid_upos = {"NOUN", "PROPN", "VERB", "PRON", "AUX", "CCONJ", "SCONJ", "CONJ", "ADV", "ADJ"}
#         self.valid_xpos = {}  # {"ncn", "nq", "ncpa", "xsv", "ep", "ef", "jp", "px", "etm"}

#     def is_valid_word(self, word):
#         # í˜•íƒœì†Œ ë¶„í• 
#         xpos_parts = word.xpos.split("+")

#         # ì¡°ê±´ ì²´í¬
#         has_valid_deprel = word.deprel in self.valid_deprels
#         has_valid_upos = word.upos in self.valid_upos
#         has_valid_xpos = any(x in self.valid_xpos for x in xpos_parts)

#         # êµì°¨ ì¡°ê±´ 2ê°œ ì´ìƒ ë§Œì¡± ì‹œ í¬í•¨
#         return sum([has_valid_deprel, has_valid_upos, has_valid_xpos]) >= 2

#     def clean_summary(self, summary: str) -> list:
#         doc = nlp(summary)

#         for sentence in doc.sentences:
#             important_words = []
#             for word in sentence.words:
#                 # print(f"{word.text}\t{word.deprel}\t{word.upos}\t{word.xpos}")

#                 if self.is_valid_word(word):
#                     important_words.append(word.text)
#                     if((word.upos in {"CONJ", "SCONJ", "CCONJ"} and word.deprel in {"conj", "root", "obl", "advcl", "cc"})):
#                     #    or \
#                         # (word.upos == "AUX" and word.deprel == "aux" and word[word.id].upos not in {"CONJ", "SCONJ", "CCONJ", "AUX"} and word.text not in {"ìˆëŠ”"})):
#                         break

#             if important_words:
#                 return " ".join(important_words)

#         return summary  # fallback
# class SentenceCleaner:
#     def __init__(self):
#         self.valid_deprels = {
#             "root", "nsubj", "obj", "iobj", "obl", "ccomp", "conj", "xcomp",
#             "advcl", "acl", "nmod", "compound", "dislocated", "dep", "aux"
#         }
#         self.valid_upos = {
#             "NOUN", "PROPN", "VERB", "PRON", "AUX", "CCONJ", "SCONJ", "CONJ", "ADV", "ADJ"
#         }
#         self.valid_xpos = set()  # ë¹„ì›Œë‘ . ì¡°ê±´ 2ê°œ ì´ìƒì´ë©´ í†µê³¼ë¨.

#         self.connecting_deprels = {"conj", "root", "obl", "advcl", "cc"}
#         self.connecting_upos = {"CONJ", "SCONJ", "CCONJ"}

#     def is_valid_word(self, word):
#         xpos_parts = word.xpos.split("+")
#         has_valid_deprel = word.deprel in self.valid_deprels
#         has_valid_upos = word.upos in self.valid_upos
#         has_valid_xpos = any(x in self.valid_xpos for x in xpos_parts)
#         return sum([has_valid_deprel, has_valid_upos, has_valid_xpos]) >= 2

#     def clean_summary(self, summary: str) -> list[str]:
#         doc = nlp(summary)

#         for sentence in doc.sentences:
#             front = []
#             back = []
#             switching = False

#             for word in sentence.words:
#                 if self.is_valid_word(word):
#                     if (word.upos in self.connecting_upos and word.deprel in self.connecting_deprels):
#                         switching = True
#                         continue  # ì—°ê²°ì–´ëŠ” ì œì™¸

#                     if switching:
#                         back.append(word.text)
#                     else:
#                         front.append(word.text)

#             return [" ".join(front).strip(), " ".join(back).strip()]

#         return [summary.strip(), ""]  # fallback: ë‚˜ëˆ„ì§€ ëª»í•œ ê²½ìš°

# class SentenceCleaner:
#     def __init__(self):
#         self.valid_deprels = {
#             "root", "nsubj", "obj", "iobj", "obl", "ccomp", "conj", "xcomp",
#             "advcl", "acl", "nmod", "compound", "dislocated", "dep", "aux"
#         }
#         self.valid_upos = {
#             "NOUN", "PROPN", "VERB", "PRON", "AUX", "CCONJ", "SCONJ", "CONJ", "ADV", "ADJ"
#         }
#         self.valid_xpos = set()

#         # clauseë¥¼ ë¶„ë¦¬í•  ë•Œ ê¸°ì¤€ì´ ë˜ëŠ” ì—°ê²° í‘œí˜„ë“¤
#         self.split_keywords = {"ê·¸ë¦¬ê³ ", "ê·¸ëŸ¬ë‚˜", "í•˜ì§€ë§Œ", "ë˜ëŠ”", "ë˜"}

#     def is_valid_word(self, word):
#         xpos_parts = word.xpos.split("+")
#         has_valid_deprel = word.deprel in self.valid_deprels
#         has_valid_upos = word.upos in self.valid_upos
#         has_valid_xpos = any(x in self.valid_xpos for x in xpos_parts)
#         return sum([has_valid_deprel, has_valid_upos, has_valid_xpos]) >= 2

#     def clean_summary(self, summary: str) -> list[str]:
#         doc = nlp(summary)

#         for sentence in doc.sentences:
#             return self.split_summary_clauses(sentence)

#         return [summary.strip(), ""]

#     def split_summary_clauses(self, sentence) -> list[str]:
#         """
#         root ì™¸ì— conj/advcl/ccompì— í•´ë‹¹í•˜ëŠ” VERBê°€ ìˆëŠ” ê²½ìš°, ê·¸ ì§€ì ì„ ê¸°ì¤€ìœ¼ë¡œ ë‚˜ëˆˆë‹¤.
#         ë˜ëŠ” 'ê·¸ë¦¬ê³ ', 'ê·¸ëŸ¬ë‚˜' ê°™ì€ ì—°ê²°ì–´ë¡œë„ ë¶„ë¦¬.
#         """
#         front = []
#         back = []
#         switching = False
#         root_ids = set()

#         # ë¨¼ì € root ë° ë³‘ë ¬ ì ˆ í›„ë³´ íƒìƒ‰
#         for word in sentence.words:
#             if word.deprel == "root":
#                 root_ids.add(word.id)
#             if word.head in root_ids and word.upos == "VERB" and word.deprel in {"conj", "advcl", "ccomp"}:
#                 root_ids.add(word.id)

#         for word in sentence.words:
#             if not self.is_valid_word(word):
#                 continue

#             # í…ìŠ¤íŠ¸ ê¸°ë°˜ ë¶„ë¦¬: ì—°ê²°ì–´
#             if word.text in self.split_keywords:
#                 switching = True
#                 continue

#             # êµ¬ì¡° ê¸°ë°˜ ë¶„ë¦¬: root ì™¸ ë³‘ë ¬ ë™ì‘ ë°œê²¬
#             if word.id in root_ids and word.deprel != "root":
#                 if switching == False:
#                     front.append(word.text)
#                 else:
#                     back.append(word.text)
#                 switching = True
#                 continue

#             if switching:
#                 back.append(word.text)
#             else:
#                 front.append(word.text)

#         return [" ".join(front).strip(), " ".join(back).strip()]


# class SummarySelector:
#     def __init__(self):
#         stanza.download("ko", processors="tokenize,pos,lemma", verbose=False)
#         self.nlp = stanza.Pipeline(
#             "ko", processors="tokenize,pos,lemma", verbose=False)

#     def get_lemmas(self, text):
#         """ë¬¸ì¥ì—ì„œ lemma(í‘œì œì–´) ì§‘í•© ì¶”ì¶œ"""
#         doc = self.nlp(text)
#         lemmas = set()
#         for sentence in doc.sentences:
#             for word in sentence.words:
#                 lemmas.add(word.lemma)
#         return lemmas

#     def jaccard_similarity(self, lemmas1, lemmas2):
#         """ë‘ ì§‘í•© ê°„ ìì¹´ë“œ ìœ ì‚¬ë„ ê³„ì‚°"""
#         intersection = lemmas1 & lemmas2
#         union = lemmas1 | lemmas2
#         if not union:
#             return 0.0
#         return len(intersection) / len(union)

#     def select_least_similar(self, summary_list, purpose, speech):
#         """ê°€ì¥ ëœ ìœ ì‚¬í•œ ìš”ì•½ë¬¸ ì„ íƒ (í‰ê·  ìœ ì‚¬ë„ 50% ë„˜ìœ¼ë©´ ë¹ˆì¹¸ ë°˜í™˜)"""
#         purpose_lemmas = self.get_lemmas(purpose)
#         speech_lemmas = self.get_lemmas(speech)

#         min_similarity = float("inf")
#         selected_summary = None

#         for summary in summary_list:
#             summary_lemmas = self.get_lemmas(summary)
#             sim_purpose = self.jaccard_similarity(
#                 summary_lemmas, purpose_lemmas)
#             sim_speech = self.jaccard_similarity(summary_lemmas, speech_lemmas)
#             avg_similarity = (sim_purpose + sim_speech) / 2

#             if avg_similarity < min_similarity:
#                 min_similarity = avg_similarity
#                 selected_summary = summary

#         # í‰ê·  ìœ ì‚¬ë„ê°€ 0.5 ì´ìƒì´ë©´ ë¹ˆì¹¸ ë°˜í™˜
#         if min_similarity >= 0.5:
#             return ""
#         if min_similarity == avg_similarity:
#             if summary_list[1] != "":
#                 return summary_list[1]

#         return selected_summary


# class Paraphraser:
#     # ëª¨ë¸ ì´ë¦„ ì„¤ì •
#     model_name = "psyche/KoT5-paraphrase-generation"

#     # pipelineì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¶ˆëŸ¬ì˜¤ê¸°
#     generator = pipeline("text2text-generation",
#                          model=model_name, device=0)  # device=0ì€ GPU ì‚¬ìš© ì‹œ

#     @classmethod
#     def generate(cls, prompt, max_tokens=128):
#         # í…ìŠ¤íŠ¸ ìƒì„±
#         # result = cls.generator(prompt, max_length=512, num_return_sequences=1, max_new_tokens=max_tokens)
#         result = cls.generator(prompt, max_length=512, num_return_sequences=1)
#         # print(result)

#         # ìƒì„±ëœ í…ìŠ¤íŠ¸ ë°˜í™˜
#         return result[0]['generated_text']

# def replace_name(name, text):
#     """
#     í…ìŠ¤íŠ¸ì—ì„œ ì£¼ì–´ì§„ ì´ë¦„ì„ í’€ë„¤ì„ìœ¼ë¡œ êµì²´
#     ì„±ì”¨ ë’¤ì— ì§ì±…ì´ë‚˜ ì§ìœ„ê°€ ìˆì„ ê²½ìš° í•´ë‹¹ ì§ìœ„ë„ í•¨ê»˜ êµì²´
#     """
#     full_name = name
#     position_words = [
#         "ëŒ€í†µë ¹", "êµ­íšŒì˜ì›", "êµ­íšŒìœ„ì›", "ìµœê³ ìœ„ì›", "ë‹¹ëŒ€í‘œ", "ëŒ€ë³€ì¸", "ë¶€ëŒ€í‘œ", "ë¹„ë¡€ëŒ€í‘œ", "ì›ë‚´ëŒ€í‘œ", "ì „ ëŒ€í‘œ",
#         "ì „ ì˜ì›", "ë‹¹ìˆ˜", "ì‹œì˜ì›", "ì§€ë°©ì˜íšŒ ì˜ì›", "ì¥ê´€", "ë¶€ì¥ê´€", "ì²­ì™€ëŒ€ ë¹„ì„œì‹¤ì¥", "ì²­ì™€ëŒ€ ëŒ€ë³€ì¸", "í†µì¼ë¶€ ì¥ê´€",
#         "ê²½ì œë¶€ì´ë¦¬", "ì¸ì‚¬í˜ì‹ ì²˜ì¥", "ì™¸êµë¶€ ì¥ê´€", "ë²•ë¬´ë¶€ ì¥ê´€", "êµìœ¡ë¶€ ì¥ê´€", "ë…¸ë™ë¶€ ì¥ê´€", "ì‚¬íšŒë³µì§€ë¶€ ì¥ê´€",
#         "ì§€ë°©ìì¹˜ë‹¨ì²´ì¥", "í–‰ì •ìì¹˜ë¶€ ì¥ê´€", "ì„ ê±°ê´€ë¦¬ìœ„ì›íšŒ ìœ„ì›", "ì •ì¹˜ì¸", "ì •ë¶€ ê³ ìœ„ ê´€ê³„ì", "ì •ë‹¹ ëŒ€í‘œ", "ì •ë‹¹ ìµœê³ ìœ„ì›",
#         "ì •ë‹¹ ëŒ€ë³€ì¸", "ì •ë‹¹ ë¶€ëŒ€í‘œ",
#     ]
#     # ì˜ì¡´ êµ¬ë¬¸ ë¶„ì„
#     doc = nlp(text)
    
#     # í…ìŠ¤íŠ¸ì—ì„œ ê° ë‹¨ì–´ë¥¼ ë¶„ì„í•˜ì—¬ ì§ìœ„ê°€ ìˆì„ ê²½ìš° í’€ë„¤ì„ìœ¼ë¡œ êµì²´
#     for sentence in doc.sentences:
#         for i, word in enumerate(sentence.words):
#             if word.text == name[0] and i + 1 < len(sentence.words) and any(pos_word in sentence.words[i+1].text for pos_word in position_words):
#                 word.text = full_name

#         # ìˆ˜ì •ëœ í…ìŠ¤íŠ¸ ë°˜í™˜
#         summary = " ".join([word.text for word in sentence.words])
#         summary = re.sub(r'\s+\.', '.', string=summary)
#     return summary


def restore_names_from_original(original: str, summary: str) -> str:
    def split_words(text):
        return re.findall(r'\b\w+\b', text)

    original_words = split_words(original)
    summary_words = split_words(summary)

    # 2ë‹¨ì–´ì”© ë¬¶ì€ í›„ë³´ë“¤
    original_pairs = [(original_words[i], original_words[i+1]) for i in range(len(original_words) - 1)]
    summary_pairs = [(summary_words[i], summary_words[i+1]) for i in range(len(summary_words) - 1)]

    # ë§¤í•‘ëœ short â†’ full ë”•ì…”ë„ˆë¦¬
    replacement_map = {}

    for o1, o2 in original_pairs:
        for s1, s2 in summary_pairs:
            # short: ê¹€ ì˜ì› / full: ê¹€ì² ìˆ˜ ì˜ì›
            if o1[0] == s1[0] and o2 == s2:
                short_form = f"{s1} {s2}"
                full_form = f"{o1} {o2}"
                replacement_map[short_form] = full_form

    # ì‹¤ì œ êµì²´ ìˆ˜í–‰
    for short, full in replacement_map.items():
        summary = summary.replace(short, full)

    return summary

class TopicExtractor:
    def __init__(self):
        self.summarizer = Summarizer()
        # self.cleaner = SentenceCleaner()
        # self.paraphraser = Paraphraser()
        # self.selector = SummarySelector()
        # self.paraphraser2 = Paraphrase()

    def extract_topic(self, title, body, purpose, sentence, name):
        summary = self.summarizer.summarize(body)
        print(f"\nìš”ì•½ ê²°ê³¼:\t{summary}")

        # cleaned = self.cleaner.clean_summary(summary)
        # print(f"ìˆ˜ì‹ì–´ ì œê±°:\t{cleaned}")

        # if cleaned[1] != "":
        #     selected = self.selector.select_least_similar(
        #         cleaned, purpose, sentence)
        # else:
        #     selected = cleaned[0]
        
        replaced = restore_names_from_original(body, summary)

        # paraphrased = self.paraphraser.generate(cleaned)
        # print(f"paraphrase:\t{paraphrased}")

        return replaced


# ğŸ” ì˜ˆì‹œ ì‹¤í–‰
if __name__ == "__main__":
    title = "ê¹€ ì˜ì›, ì¥ì• ì¸ì˜ˆìˆ ë‹¨ ì„¤ë¦½ ì§ˆì˜"
    body1 = """
    í†µí•©ë‹¹ ê°„ì‚¬ì¸ ì´ì±„ìµ ì˜ì›ì€ ì½”ë¡œë‚˜19 ìê°€ê²©ë¦¬ìì—ê²Œ ê±°ì†Œíˆ¬í‘œÂ·ì„ ìƒíˆ¬í‘œë¥¼ í—ˆìš©í•˜ëŠ” ë“± ëŒ€ì±… ë§ˆë ¨ì„ ì£¼ë¬¸í–ˆê³ , ì´ìŠ¹íƒ í›„ë³´ìëŠ” "ë³´ê±´ë‹¹êµ­ì˜ ì´ë™ì œí•œ í—ˆìš©ì„ ì „ì œë¡œ ì‚¬ì „íˆ¬í‘œê°€ ê°€ëŠ¥í•  ê²ƒ ê°™ë‹¤"ë©´ì„œ "ì°¸ì •ê¶Œ í™•ëŒ€ë¼ëŠ” ë¶€ë¶„ê³¼ ê´€ë ¨í•´ì„œ ì ê·¹ ì˜ê²¬ì„ ê°œì§„í•˜ê² ë‹¤"ê³  ë‹µí–ˆë‹¤.
ë¯¼ì£¼ë‹¹ ê¶Œë¯¸í˜ ì˜ì›ì€ "ì „ìê±°ì†Œíˆ¬í‘œ ë„ì…ì„ ê²€í† í•´ì•¼ í•œë‹¤"ë©° ì„ ê´€ìœ„ì˜ ì˜¨ë¼ì¸íˆ¬í‘œì‹œìŠ¤í…œì¸ 'ì¼€ì´ë³´íŒ…'(K-voting) ì´ìš© ë°©ì•ˆì„ ì œì•ˆí–ˆë‹¤.
    """
#     body2 = """
#     ê¹€í˜„ê¶Œ êµ­íšŒì˜ì›(ë”ë¶ˆì–´ë¯¼ì£¼ë‹¹Â·ë¹„ë¡€ëŒ€í‘œÂ·ì‚¬ì§„)ì€ "ìµœê·¼ êµ­ë°©ë¶€ì˜ í†µí•©ì‹ ê³µí•­ ë¶€ì§€ì„ ì • ë°œí‘œë¥¼ í™˜ì˜í•œë‹¤"ë©´ì„œ "ì•ìœ¼ë¡œ êµ¬ë¯¸ì‹œë¥¼ ì‹ ê³µí•­ ë°°í›„ êµí†µÂ·ë¬¼ë¥˜Â·ì‚°ì—…ì˜ ì¤‘ì‹¬ì§€ë¡œ ì»¤ ë‚˜ê°€ë„ë¡ ì§€ì›ì„ ì•„ë¼ì§€ ì•Šê² ë‹¤"ê³  30ì¼ ë°í˜”ë‹¤.
#     """
#     body3 = """
#     ì£¼ë³€ ë„ì‹œë¥¼ ì‡ëŠ” êµí†µë§ í™•ì¶© ì—­ì‹œ ì‹ ê³µí•­ì˜ ì„±íŒ¨ë¥¼ ì¢Œìš°í•  í•µì‹¬ê³¼ì œë¡œ ë– ì˜¤ë¥´ê³  ìˆë‹¤. ê²½ë¶ë„ì— ë”°ë¥´ë©´ 2021ë…„ë¶€í„° ì „ì²  4ê³³, ê³ ì†ë„ë¡œ 2ê³³ ë“± ì´ 260ãì— ê±¸ì³ êµ­ë¹„ 6ì¡°ì›ì„ íˆ¬ì…í•˜ëŠ” ì‹ ê³µí•­ê³¼ êµ¬ë¯¸Â·í¬í•­Â·ëŒ€êµ¬ ë“± ì¸ê·¼ ë„ì‹œë“¤ì„ ì—°ê²°í•˜ëŠ” êµí†µë§ í™•ì¶©ì‚¬ì—…ì´ ì¶”ì§„ëœë‹¤.
# ê¹€ ì˜ì›ì€ "êµ¬ë¯¸ì‹œê°€ ì‹ ê³µí•­ë°°í›„ë‹¨ì§€ë¡œì„œ ì‚°ì—…Â·êµí†µÂ·ë¬¼ë¥˜ì˜ ì¤‘ì‹¬ì§€ë¡œ ë¶€ìƒí•˜ë©´ êµ¬ë¯¸ì‚°ë‹¨ì´ë‚˜ ì•„íŒŒíŠ¸ ì‹ ë„ì‹œ í™œì„±í™”ë¿ë§Œ ì•„ë‹ˆë¼ ë„ì‹œì™€ ë†ì´Œì´ ì¡°í™”í•˜ëŠ” ì§€ì—­ ê· í˜•ë°œì „ì´ ì´ë¤„ì§ˆ ê²ƒ"ì´ë¼ê³  ë‚´ë‹¤ë´¤ë‹¤.
#     """
    extractor = TopicExtractor()
    topic = extractor.extract_topic(title, body1)
    # topic = extractor.extract_topic(title, body2)
    # topic = extractor.extract_topic(title, body3)
